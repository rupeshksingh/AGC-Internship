{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4732f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_1 = pd.read_csv(\"Data\\labelled_2_37.csv\")\n",
    "df_2 = pd.read_csv(\"Data\\labelled_1_23.csv\")\n",
    "df_3 = pd.read_csv(\"Data\\labelled_0_44.csv\")\n",
    "df_4 = pd.read_csv(\"Data\\AGC_Data.csv\")\n",
    "\n",
    "df = pd.concat([df_3, df_2, df_4, df_1], axis=0)\n",
    "df.to_csv(\"AGC_Combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4185525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    make_scorer\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "932c43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_time_series_features(df, speed_col='speed', timestamp_col='indo_time'):\n",
    "    \"\"\"\n",
    "    Enhanced feature engineering specifically for time series anomaly detection.\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(df[speed_col]).copy()\n",
    "    features.columns = ['Speed']\n",
    "\n",
    "    windows = {\n",
    "        'ultra_short': 30,\n",
    "        'short': 60,\n",
    "        'medium': 300,\n",
    "        'long': 900,\n",
    "        'ultra_long': 1800\n",
    "    }\n",
    "    \n",
    "    # 1. Enhanced Rolling Statistics\n",
    "    for name, window in windows.items():\n",
    "        min_periods = max(1, int(window * 0.1))\n",
    "        \n",
    "        # Basic statistics\n",
    "        features[f'Speed_rolling_mean_{window}'] = (\n",
    "            df[speed_col].rolling(window=window, min_periods=min_periods).mean()\n",
    "        )\n",
    "        features[f'Speed_rolling_std_{window}'] = (\n",
    "            df[speed_col].rolling(window=window, min_periods=min_periods).std().fillna(0)\n",
    "        )\n",
    "        features[f'Speed_rolling_median_{window}'] = (\n",
    "            df[speed_col].rolling(window=window, min_periods=min_periods).median()\n",
    "        )\n",
    "        \n",
    "        # Advanced statistics\n",
    "        features[f'Speed_rolling_skew_{window}'] = (\n",
    "            df[speed_col].rolling(window=window, min_periods=min_periods).skew().fillna(0)\n",
    "        )\n",
    "        features[f'Speed_rolling_kurt_{window}'] = (\n",
    "            df[speed_col].rolling(window=window, min_periods=min_periods).kurt().fillna(0)\n",
    "        )\n",
    "        \n",
    "        # Range-based features\n",
    "        rolling_min = df[speed_col].rolling(window=window, min_periods=min_periods).min()\n",
    "        rolling_max = df[speed_col].rolling(window=window, min_periods=min_periods).max()\n",
    "        features[f'Speed_rolling_range_{window}'] = rolling_max - rolling_min\n",
    "        \n",
    "        # Coefficient of variation\n",
    "        features[f'Speed_rolling_cv_{window}'] = (\n",
    "            features[f'Speed_rolling_std_{window}'] / \n",
    "            (features[f'Speed_rolling_mean_{window}'] + 1e-9)\n",
    "        )\n",
    "        \n",
    "        # Z-score\n",
    "        features[f'Speed_rolling_zscore_{window}'] = (\n",
    "            (df[speed_col] - features[f'Speed_rolling_mean_{window}']) /\n",
    "            (features[f'Speed_rolling_std_{window}'] + 1e-9)\n",
    "        )\n",
    "        \n",
    "        # Mean-median difference (asymmetry indicator)\n",
    "        features[f'Speed_mean_median_diff_{window}'] = (\n",
    "            features[f'Speed_rolling_mean_{window}'] - features[f'Speed_rolling_median_{window}']\n",
    "        )\n",
    "        \n",
    "        # Percentiles\n",
    "        features[f'Speed_rolling_q10_{window}'] = (\n",
    "            df[speed_col].rolling(window=window, min_periods=min_periods).quantile(0.1)\n",
    "        )\n",
    "        features[f'Speed_rolling_q90_{window}'] = (\n",
    "            df[speed_col].rolling(window=window, min_periods=min_periods).quantile(0.9)\n",
    "        )\n",
    "        features[f'Speed_rolling_iqr_{window}'] = (\n",
    "            df[speed_col].rolling(window=window, min_periods=min_periods).quantile(0.75) -\n",
    "            df[speed_col].rolling(window=window, min_periods=min_periods).quantile(0.25)\n",
    "        )\n",
    "    \n",
    "    # 2. Temporal Change Features\n",
    "    for lag in [1, 5, 10, 30, 60, 120]:\n",
    "        features[f'Speed_diff_{lag}'] = df[speed_col].diff(periods=lag).fillna(0)\n",
    "        features[f'Speed_pct_change_{lag}'] = df[speed_col].pct_change(periods=lag).fillna(0)\n",
    "        features[f'Speed_lag_{lag}'] = df[speed_col].shift(lag).fillna(df[speed_col].mean())\n",
    "    \n",
    "    # Acceleration and jerk\n",
    "    features['Speed_acceleration'] = df[speed_col].diff().diff().fillna(0)\n",
    "    features['Speed_jerk'] = df[speed_col].diff().diff().diff().fillna(0)\n",
    "    \n",
    "    # 3. Exponential Weighted Features (multiple spans)\n",
    "    for span in [10, 30, 60, 120]:\n",
    "        alpha = 2 / (span + 1)\n",
    "        features[f'Speed_ewm_mean_span_{span}'] = (\n",
    "            df[speed_col].ewm(span=span, adjust=False, min_periods=1).mean()\n",
    "        )\n",
    "        features[f'Speed_ewm_std_span_{span}'] = (\n",
    "            df[speed_col].ewm(span=span, adjust=False, min_periods=1).std().fillna(0)\n",
    "        )\n",
    "        features[f'Speed_ewm_diff_span_{span}'] = (\n",
    "            df[speed_col] - features[f'Speed_ewm_mean_span_{span}']\n",
    "        )\n",
    "    \n",
    "    # 4. CUSUM Features (both positive and negative)\n",
    "    for window in [60, 300]:\n",
    "        mean_col = f'Speed_rolling_mean_{window}'\n",
    "        # Positive deviations\n",
    "        features[f'Speed_positive_deviation_{window}'] = np.maximum(\n",
    "            0, df[speed_col] - features[mean_col]\n",
    "        )\n",
    "        features[f'Speed_cusum_positive_{window}'] = (\n",
    "            features[f'Speed_positive_deviation_{window}']\n",
    "            .rolling(window=window, min_periods=1)\n",
    "            .sum()\n",
    "        )\n",
    "        \n",
    "        # Negative deviations\n",
    "        features[f'Speed_negative_deviation_{window}'] = np.maximum(\n",
    "            0, features[mean_col] - df[speed_col]\n",
    "        )\n",
    "        features[f'Speed_cusum_negative_{window}'] = (\n",
    "            features[f'Speed_negative_deviation_{window}']\n",
    "            .rolling(window=window, min_periods=1)\n",
    "            .sum()\n",
    "        )\n",
    "    \n",
    "    # 5. Time-based Features (if timestamp available)\n",
    "    if timestamp_col in df.columns:\n",
    "        # Extract time components\n",
    "        features['hour'] = pd.to_datetime(df[timestamp_col]).dt.hour\n",
    "        features['day_of_week'] = pd.to_datetime(df[timestamp_col]).dt.dayofweek\n",
    "        features['is_weekend'] = features['day_of_week'].isin([5, 6]).astype(int)\n",
    "        \n",
    "        # Cyclic encoding\n",
    "        features['hour_sin'] = np.sin(2 * np.pi * features['hour'] / 24)\n",
    "        features['hour_cos'] = np.cos(2 * np.pi * features['hour'] / 24)\n",
    "        features['dow_sin'] = np.sin(2 * np.pi * features['day_of_week'] / 7)\n",
    "        features['dow_cos'] = np.cos(2 * np.pi * features['day_of_week'] / 7)\n",
    "    \n",
    "    # 6. Entropy-based Features (measure of randomness)\n",
    "    def rolling_entropy(series, window):\n",
    "        def entropy(x):\n",
    "            if len(x) < 2:\n",
    "                return 0\n",
    "            hist, _ = np.histogram(x, bins=10)\n",
    "            hist = hist[hist > 0]\n",
    "            if len(hist) == 0:\n",
    "                return 0\n",
    "            prob = hist / hist.sum()\n",
    "            return -np.sum(prob * np.log2(prob + 1e-9))\n",
    "        \n",
    "        return series.rolling(window=window, min_periods=max(1, window//10)).apply(entropy, raw=True)\n",
    "    \n",
    "    for window in [60, 300]:\n",
    "        features[f'Speed_entropy_{window}'] = rolling_entropy(df[speed_col], window).fillna(0)\n",
    "    \n",
    "    # 7. Autoregressive Features\n",
    "    for i in range(1, 6):\n",
    "        features[f'Speed_ar_{i}'] = df[speed_col].shift(i).fillna(df[speed_col].mean())\n",
    "    \n",
    "    # 8. Moving Average Crossover Features\n",
    "    features['Speed_ma_crossover_short_medium'] = (\n",
    "        (features['Speed_rolling_mean_60'] > features['Speed_rolling_mean_300']).astype(int)\n",
    "    )\n",
    "    features['Speed_ma_crossover_medium_long'] = (\n",
    "        (features['Speed_rolling_mean_300'] > features['Speed_rolling_mean_900']).astype(int)\n",
    "    )\n",
    "\n",
    "    features = features.fillna(features.mean())\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93034ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_scorer():\n",
    "    \"\"\"\n",
    "    Create a custom scorer that balances between recall for anomalies and overall accuracy.\n",
    "    \"\"\"\n",
    "    def anomaly_aware_score(y_true, y_pred):\n",
    "        f1_anomaly = f1_score(y_true, y_pred, pos_label=1, average='binary')\n",
    "        bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "        return 0.7 * f1_anomaly + 0.3 * bal_acc\n",
    "    \n",
    "    return make_scorer(anomaly_aware_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a587731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomaly_range_ml_enhanced(\n",
    "    df,\n",
    "    speed_col='speed',\n",
    "    label_col='pred_label',\n",
    "    timestamp_col='indo_time',\n",
    "    random_state=42,\n",
    "    save_path_prefix='anomaly_interval',\n",
    "    use_hyperparameter_tuning=True,\n",
    "    n_iter=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced anomaly detection with hyperparameter tuning and advanced features.\n",
    "    \"\"\"\n",
    "    print(\"Building enhanced features for time series anomaly detection...\")\n",
    "    X = engineer_time_series_features(df, speed_col, timestamp_col)\n",
    "    y = df[label_col]\n",
    "    \n",
    "    print(f\"Total features created: {X.shape[1]}\")\n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=0.20, stratify=y, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nShapes → Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    print(\"Anomaly distribution in TRAIN:\", y_train.value_counts(normalize=True).to_dict())\n",
    "    print(\"Anomaly distribution in VALID:\", y_val.value_counts(normalize=True).to_dict())\n",
    "    print(\"Anomaly distribution in TEST:\", y_test.value_counts(normalize=True).to_dict())\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "    X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "    neg_count = y_train.value_counts().get(0, 0)\n",
    "    pos_count = y_train.value_counts().get(1, 0)\n",
    "    scale_pos_weight = (neg_count / pos_count) if (pos_count > 0) else 1\n",
    "    \n",
    "    if use_hyperparameter_tuning:\n",
    "        print(\"\\nPerforming hyperparameter tuning...\")\n",
    "\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300, 500],\n",
    "            'max_depth': [3, 5, 7, 10, 15, None],\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "            'min_child_weight': [1, 3, 5, 7, 10],\n",
    "            'gamma': [0, 0.1, 0.2, 0.3, 0.5],\n",
    "            'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'reg_alpha': [0, 0.001, 0.01, 0.1, 1],\n",
    "            'reg_lambda': [0, 0.001, 0.01, 0.1, 1],\n",
    "            'scale_pos_weight': [scale_pos_weight, scale_pos_weight * 0.8, scale_pos_weight * 1.2]\n",
    "        }\n",
    "\n",
    "        base_model = XGBClassifier(\n",
    "            random_state=random_state,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            early_stopping_rounds=20,\n",
    "            n_jobs=4\n",
    "        )\n",
    "   \n",
    "        scorer = create_custom_scorer()\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "   \n",
    "        random_search = RandomizedSearchCV(\n",
    "            base_model,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=n_iter,\n",
    "            scoring=scorer,\n",
    "            cv=tscv,\n",
    "            verbose=1,\n",
    "            random_state=random_state,\n",
    "            n_jobs=4\n",
    "        )\n",
    "\n",
    "        random_search.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            eval_set=[(X_val_scaled, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nBest parameters: {random_search.best_params_}\")\n",
    "        print(f\"Best CV score: {random_search.best_score_:.4f}\")\n",
    "        \n",
    "        model = random_search.best_estimator_\n",
    "    else:\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=7,\n",
    "            learning_rate=0.1,\n",
    "            min_child_weight=3,\n",
    "            gamma=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=random_state,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            n_jobs=4\n",
    "        )\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    print(\"\\n--- MODEL EVALUATION ON VALIDATION SPLIT ---\")\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    y_val_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "    \n",
    "    print(f\"Accuracy (Val): {accuracy_score(y_val, y_val_pred):.4f}\")\n",
    "    print(f\"Balanced Accuracy (Val): {balanced_accuracy_score(y_val, y_val_pred):.4f}\")\n",
    "    print(f\"F1 Score - Anomaly Class (Val): {f1_score(y_val, y_val_pred, pos_label=1):.4f}\")\n",
    "    try:\n",
    "        print(f\"ROC AUC (Val): {roc_auc_score(y_val, y_val_proba):.4f}\")\n",
    "    except ValueError:\n",
    "        print(\"ROC AUC (Val): N/A\")\n",
    "    \n",
    "    print(\"\\nClassification Report (Val):\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "    print(\"Confusion Matrix (Val):\")\n",
    "    print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "    print(\"\\n--- TOP 20 FEATURE IMPORTANCES ---\")\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False).head(20)\n",
    "    print(feature_importances.to_string(index=False))\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_features = feature_importances.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 15 Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_path_prefix}_feature_importances.png\")\n",
    "    plt.close()\n",
    "\n",
    "    print(\"\\nPredicting on TEST split and analyzing anomaly intervals...\")\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    y_test_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    print(f\"\\nTest Set Performance:\")\n",
    "    print(f\"Accuracy (Test): {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"Balanced Accuracy (Test): {balanced_accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"F1 Score - Anomaly Class (Test): {f1_score(y_test, y_test_pred, pos_label=1):.4f}\")\n",
    "\n",
    "    test_idx = X_test.index.values\n",
    "    test_df = pd.DataFrame({\n",
    "        'orig_idx': test_idx,\n",
    "        'pred': y_test_pred,\n",
    "        'proba': y_test_proba\n",
    "    }).sort_values(by='orig_idx').reset_index(drop=True)\n",
    "\n",
    "    intervals_idx = []\n",
    "    in_anomaly = False\n",
    "    start_idx = None\n",
    "    \n",
    "    for row in test_df.itertuples(index=False):\n",
    "        orig_i = row.orig_idx\n",
    "        label = row.pred\n",
    "        \n",
    "        if (label == 1) and (not in_anomaly):\n",
    "            in_anomaly = True\n",
    "            start_idx = orig_i\n",
    "        elif (label == 0) and in_anomaly:\n",
    "            intervals_idx.append((start_idx, prev_i))\n",
    "            in_anomaly = False\n",
    "        \n",
    "        prev_i = orig_i\n",
    "    \n",
    "    if in_anomaly:\n",
    "        intervals_idx.append((start_idx, prev_i))\n",
    "    \n",
    "    if not intervals_idx:\n",
    "        print(\"No predicted anomaly intervals in TEST split.\")\n",
    "        return [], model, scaler\n",
    "\n",
    "    intervals_ts = []\n",
    "    for (s_idx, e_idx) in intervals_idx:\n",
    "        s_ts = df.loc[s_idx, timestamp_col]\n",
    "        e_ts = df.loc[e_idx, timestamp_col]\n",
    "        intervals_ts.append((s_ts, e_ts))\n",
    "\n",
    "    context_window = 60\n",
    "    for i, (s_idx, e_idx) in enumerate(intervals_idx[:5]):\n",
    "        start_context = max(0, s_idx - context_window)\n",
    "        end_context = min(len(df) - 1, e_idx + context_window)\n",
    "        \n",
    "        df_seg = df.loc[start_context:end_context].copy()\n",
    "\n",
    "        seg_indices = df_seg.index\n",
    "        seg_probas = test_df[test_df['orig_idx'].isin(seg_indices)]['proba'].values\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), height_ratios=[2, 1])\n",
    "\n",
    "        sc = ax1.scatter(\n",
    "            df_seg[timestamp_col],\n",
    "            df_seg[speed_col],\n",
    "            c=df_seg[label_col],\n",
    "            cmap='coolwarm',\n",
    "            s=15,\n",
    "            edgecolors='none',\n",
    "            alpha=0.7\n",
    "        )\n",
    "        \n",
    "        s_ts = df.loc[s_idx, timestamp_col]\n",
    "        e_ts = df.loc[e_idx, timestamp_col]\n",
    "        \n",
    "        ax1.axvline(x=s_ts, color='green', linestyle='--', linewidth=1.2, label='Anomaly Start')\n",
    "        ax1.axvline(x=e_ts, color='red', linestyle='--', linewidth=1.2, label='Anomaly End')\n",
    "        ax1.set_ylabel('Speed')\n",
    "        ax1.set_title(f'Test Interval {i+1}: {s_ts} → {e_ts}')\n",
    "        ax1.legend(loc='upper right')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        if len(seg_probas) > 0:\n",
    "            ax2.plot(df_seg[timestamp_col][:len(seg_probas)], seg_probas, \n",
    "                    color='purple', linewidth=1.5, alpha=0.8)\n",
    "            ax2.axhline(y=0.5, color='black', linestyle=':', alpha=0.5)\n",
    "            ax2.fill_between(df_seg[timestamp_col][:len(seg_probas)], \n",
    "                           seg_probas, 0.5, where=(seg_probas > 0.5),\n",
    "                           color='red', alpha=0.3, label='Anomaly Region')\n",
    "        \n",
    "        ax2.set_xlabel('Time')\n",
    "        ax2.set_ylabel('Anomaly Probability')\n",
    "        ax2.set_ylim(-0.05, 1.05)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        filename = f\"{save_path_prefix}_test_interval_{i+1}_enhanced.png\"\n",
    "        plt.savefig(filename, dpi=150)\n",
    "        plt.close()\n",
    "        print(f\"Saved enhanced plot: {filename}\")\n",
    "    \n",
    "    print(f\"\\nFinal Result: Found {len(intervals_ts)} anomaly ranges in TEST split.\")\n",
    "    \n",
    "    return intervals_ts, model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54072b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_anomaly_model(model, scaler, feature_columns, model_path='anomaly_detection_model.pkl'):\n",
    "    \"\"\"\n",
    "    Save the trained model, scaler, and feature information.\n",
    "    \"\"\"\n",
    "    model_data = {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'feature_columns': feature_columns,\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'model_params': model.get_params()\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_data, model_path)\n",
    "    print(f\"Model saved successfully to: {model_path}\")\n",
    "    \n",
    "    # Also save feature columns separately for reference\n",
    "    with open('model_features.txt', 'w') as f:\n",
    "        f.write(f\"Model trained on {len(feature_columns)} features\\n\")\n",
    "        f.write(f\"Timestamp: {model_data['timestamp']}\\n\\n\")\n",
    "        f.write(\"Feature list:\\n\")\n",
    "        for i, feature in enumerate(feature_columns, 1):\n",
    "            f.write(f\"{i}. {feature}\\n\")\n",
    "    \n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_anomaly_model(model_path='anomaly_detection_model.pkl'):\n",
    "    \"\"\"\n",
    "    Load the saved model and associated data.\n",
    "    \"\"\"\n",
    "    model_data = joblib.load(model_path)\n",
    "    print(f\"Model loaded from: {model_path}\")\n",
    "    print(f\"Model was trained on: {model_data['timestamp']}\")\n",
    "    print(f\"Number of features: {len(model_data['feature_columns'])}\")\n",
    "    \n",
    "    return model_data['model'], model_data['scaler'], model_data['feature_columns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_new_data(file_path, speed_col='A2:MCPGSpeed', timestamp_col='Time_stamp'):\n",
    "    \"\"\"\n",
    "    Load and prepare new data for prediction.\n",
    "    \"\"\"\n",
    "    # Read the new data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure timestamp is datetime\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values(by=timestamp_col).reset_index(drop=True)\n",
    "    \n",
    "    # Rename columns to match training data format\n",
    "    df_prepared = pd.DataFrame({\n",
    "        'speed': df[speed_col],\n",
    "        'indo_time': df[timestamp_col]\n",
    "    })\n",
    "    \n",
    "    print(f\"Loaded {len(df_prepared)} rows of new data\")\n",
    "    print(f\"Date range: {df_prepared['indo_time'].min()} to {df_prepared['indo_time'].max()}\")\n",
    "    print(f\"Speed range: {df_prepared['speed'].min():.1f} to {df_prepared['speed'].max():.1f}\")\n",
    "    \n",
    "    return df_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded: 246001 rows, 4 columns\n",
      "Building enhanced features for time series anomaly detection...\n",
      "Total features created: 117\n",
      "\n",
      "Shapes → Train: (147600, 117), Val: (49200, 117), Test: (49201, 117)\n",
      "Anomaly distribution in TRAIN: {0: 0.940860433604336, 1: 0.05913956639566396}\n",
      "Anomaly distribution in VALID: {0: 0.9408739837398374, 1: 0.0591260162601626}\n",
      "Anomaly distribution in TEST: {0: 0.9408751854637101, 1: 0.05912481453628991}\n",
      "\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "\n",
      "Best parameters: {'subsample': 0.7, 'scale_pos_weight': np.float64(15.909153396723566), 'reg_lambda': 0.01, 'reg_alpha': 0.1, 'n_estimators': 300, 'min_child_weight': 3, 'max_depth': None, 'learning_rate': 0.3, 'gamma': 0.1, 'colsample_bytree': 0.7}\n",
      "Best CV score: 0.9971\n",
      "\n",
      "--- MODEL EVALUATION ON VALIDATION SPLIT ---\n",
      "Accuracy (Val): 0.9997\n",
      "Balanced Accuracy (Val): 0.9990\n",
      "F1 Score - Anomaly Class (Val): 0.9976\n",
      "ROC AUC (Val): 1.0000\n",
      "\n",
      "Classification Report (Val):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     46291\n",
      "           1       1.00      1.00      1.00      2909\n",
      "\n",
      "    accuracy                           1.00     49200\n",
      "   macro avg       1.00      1.00      1.00     49200\n",
      "weighted avg       1.00      1.00      1.00     49200\n",
      "\n",
      "Confusion Matrix (Val):\n",
      "[[46282     9]\n",
      " [    5  2904]]\n",
      "\n",
      "--- TOP 20 FEATURE IMPORTANCES ---\n",
      "                    Feature  Importance\n",
      "        Speed_rolling_cv_60    0.256174\n",
      "    Speed_rolling_range_300    0.182842\n",
      "      Speed_ewm_std_span_30    0.071377\n",
      "       Speed_rolling_std_60    0.058621\n",
      "          Speed_entropy_300    0.046666\n",
      "     Speed_rolling_range_60    0.026492\n",
      "       Speed_rolling_std_30    0.025831\n",
      "    Speed_rolling_skew_1800    0.025316\n",
      "        Speed_rolling_cv_30    0.024519\n",
      "     Speed_rolling_kurt_300    0.023535\n",
      "   Speed_rolling_range_1800    0.016109\n",
      "      Speed_ewm_std_span_60    0.015856\n",
      "     Speed_rolling_range_30    0.013394\n",
      "      Speed_rolling_q10_300    0.012822\n",
      "      Speed_rolling_std_900    0.011624\n",
      "     Speed_ewm_std_span_120    0.010690\n",
      "Speed_mean_median_diff_1800    0.010161\n",
      "   Speed_cusum_negative_300    0.009564\n",
      "     Speed_rolling_skew_900    0.008423\n",
      "      Speed_rolling_cv_1800    0.007949\n",
      "\n",
      "Predicting on TEST split and analyzing anomaly intervals...\n",
      "\n",
      "Test Set Performance:\n",
      "Accuracy (Test): 0.9999\n",
      "Balanced Accuracy (Test): 0.9996\n",
      "F1 Score - Anomaly Class (Test): 0.9995\n",
      "Saved enhanced plot: anomaly_interval_test_interval_1_enhanced.png\n",
      "Saved enhanced plot: anomaly_interval_test_interval_2_enhanced.png\n",
      "Saved enhanced plot: anomaly_interval_test_interval_3_enhanced.png\n",
      "Saved enhanced plot: anomaly_interval_test_interval_4_enhanced.png\n",
      "Saved enhanced plot: anomaly_interval_test_interval_5_enhanced.png\n",
      "\n",
      "Final Result: Found 33 anomaly ranges in TEST split.\n",
      "\n",
      "Detected anomaly intervals:\n",
      "  1. 2024-04-12 03:46:28+07:00 → 2024-04-12 03:55:44+07:00\n",
      "  2. 2024-07-13 11:14:00+07:00 → 2024-07-13 11:17:39+07:00\n",
      "  3. 2024-07-13 15:48:58+07:00 → 2024-07-13 15:54:12+07:00\n",
      "  4. 2024-07-14 00:00:11+07:00 → 2024-07-14 00:10:38+07:00\n",
      "  5. 2024-07-14 01:00:42+07:00 → 2024-07-14 01:01:54+07:00\n",
      "  6. 2024-07-14 01:02:00+07:00 → 2024-07-14 01:02:02+07:00\n",
      "  7. 2024-07-14 08:24:05+07:00 → 2024-07-14 08:24:54+07:00\n",
      "  8. 2024-07-14 09:30:35+07:00 → 2024-07-14 09:36:37+07:00\n",
      "  9. 2024-07-14 13:41:49+07:00 → 2024-07-14 13:47:40+07:00\n",
      "  10. 2024-07-15 11:00:29+07:00 → 2024-07-15 11:09:25+07:00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = 'Data\\AGC_Combined.csv'\n",
    "    try:\n",
    "        print(\"Loading data...\")\n",
    "        data_df = pd.read_csv(file_path, parse_dates=['indo_time'])\n",
    "        print(f\"Data loaded: {data_df.shape[0]} rows, {data_df.shape[1]} columns\")\n",
    "\n",
    "        intervals_ts, model, scaler = detect_anomaly_range_ml_enhanced(\n",
    "            df=data_df,\n",
    "            speed_col='speed',\n",
    "            label_col='pred_label',\n",
    "            timestamp_col='indo_time',\n",
    "            random_state=42,\n",
    "            save_path_prefix='anomaly_interval',\n",
    "            use_hyperparameter_tuning=True,\n",
    "            n_iter=10\n",
    "        )\n",
    "        \n",
    "        if intervals_ts:\n",
    "            print(f\"\\nDetected anomaly intervals:\")\n",
    "            for i, (start, end) in enumerate(intervals_ts[:10], 1):\n",
    "                print(f\"  {i}. {start} → {end}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
